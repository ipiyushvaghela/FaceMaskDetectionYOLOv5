{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/content/drive/MyDrive/face_mask_detection/face_data/train/images\"\n",
    "val_path = \"/content/drive/MyDrive/face_mask_detection/face_data/val/images\"\n",
    "test_path = \"/content/drive/MyDrive/face_mask_detection/face_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clonning git hub repo of yolov5\n",
    "\n",
    "# !git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images should be structured in following formate...\n",
    "\n",
    "data -->\n",
    "    \n",
    "    train --> images --> .jpg image\n",
    "    train --> labels --> .txt files\n",
    "    \n",
    "    test --> images --> .jpg image\n",
    "    test --> labels --> .txt files\n",
    "\n",
    "update all the paths in dataset.yaml file and then put .yaml file straight into cloned yolov5 github repo.\n",
    "\n",
    "if weights is not given in yolov5 cloned repo then find online and put straight into cloned yolov5 github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model on pretrained model yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python train.py --img 416 --batch 8 --epochs 50 --data 'path to dataset.yaml' --weights 'path to yolov5s.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying resultant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python detect.py --source r'D:\\E\\DXAssignment\\YOLOv5Projects\\faceMask\\test' --weights r'D:\\E\\DXAssignment\\YOLOv5Projects\\yolov5\\runs\\train\\exp8\\weights\\best.pt' --img 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for imageName in glob.glob('runs\\detect\\exp4\\*.jpg'): #assuming JPG\n",
    "    display(Image(filename=imageName))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\E\\DXAssignment\\YOLOv5Projects\\venv4yolov5\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\ipiyu/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-11-18 Python-3.10.0 torch-1.13.0+cu117 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=r'D:\\E\\DXAssignment\\YOLOv5Projects\\yolov5\\runs\\train\\exp8\\weights\\best.pt', force_reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conf = 0.5  # confidence threshold (0-1)\n",
    "model.iou = 0.5  # NMS IoU threshold (0-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Make detections \n",
    "    results = model(frame)\n",
    "    \n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv4yolov5': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3055d9d68abf2b682cbf943ad6e62de017031ab3438df97fc784d05a4d8b00dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
